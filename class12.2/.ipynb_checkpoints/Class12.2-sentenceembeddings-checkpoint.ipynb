{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 12.2: Sentence Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: Installing gensim and its dependencies and launching a Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``python3 -m pip install numpy``\n",
    "\n",
    "``python3 -m pip install scipy``\n",
    "\n",
    "``python3 -m pip install gensim``\n",
    "\n",
    "``python3 -m pip install scikit-learn``\n",
    "\n",
    "``jupyter notebook``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: Getting a pre-trained word2vec model\n",
    "\n",
    "You can get a pre-trained word2vec model built on billions words of Google newsfrom here:\n",
    "\n",
    "https://github.com/eyaler/word2vec-slim/blob/master/GoogleNews-vectors-negative300-SLIM.bin.gz\n",
    "\n",
    "Just click on the \"download\" icon next to where it says \"Raw\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.spatial.distance import cosine\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and using the pre-trained word2vec model\n",
    "\n",
    "<b>Note: When you run the code below, it might take a minute to load the model!</b> Wait until you see <code>\"big model loaded\"</code> printed out below the cell. You can also check for the <code>*</code> in the brackets to the left of the cell you are executing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigmodel = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300-SLIM.bin.gz\", binary=True)\n",
    "print(\"big model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try summing some embeddings for sentences. Here's a little function that will do that for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_embed(sentence):\n",
    "    sentembed = np.zeros(300)\n",
    "    for w in sentence.split():\n",
    "        if w in bigmodel:\n",
    "            sentembed += bigmodel[w]\n",
    "    return sentembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1= \"Dogs have fur and floppy ears\"\n",
    "s2 =\"Cats are fluffy and have long tails\"\n",
    "s3 = \"Computer science is fun and easy\"\n",
    "s4 = \"Programming is an important skill\"\n",
    "s5 = \"Click here with the mouse\"\n",
    "\n",
    "sent1 = get_sent_embed(s1)\n",
    "sent2 = get_sent_embed(s2)\n",
    "sent3 = get_sent_embed(s3)\n",
    "sent4 = get_sent_embed(s4)\n",
    "sent5 = get_sent_embed(s5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allsentences = [s1, s2, s3, s4, s5]\n",
    "allw2v = [sent1, sent2, sent3, sent4, sent5]\n",
    "for i in range(len(allsentences)):\n",
    "    for j in range(len(allsentences)):\n",
    "        print(allsentences[i] + \" VS. \" + allsentences[j], end=\" ||| \")\n",
    "        print(f'{1-cosine(allw2v[i], allw2v[j]):.3f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing sentence vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below projects our sentence embeddings down to 2D and then plots them, labeled with the relevant main word of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = [sent1, sent2, sent3, sent4, sent5]\n",
    "vecwords = [\"dog\", \"cat\", \"CS\", \"programming\", \"mouse\"]\n",
    "\n",
    "    \n",
    "# Do  PCA to reduce to 2 dimensions\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "vectors2d = pca.fit(vecs).transform(vecs)\n",
    "\n",
    "# Again, ugly matplotlib code to create visualization\n",
    "i = 0\n",
    "for point, word in zip(vectors2d, vecwords):\n",
    "    plt.scatter(point[0], point[1], c='r')\n",
    "    \n",
    "    plt.annotate(\n",
    "            word, \n",
    "            xy=(point[0], point[1]),\n",
    "            xytext=(7, 6),\n",
    "            textcoords='offset points',\n",
    "            ha='left' ,\n",
    "            va='top',\n",
    "            size=\"medium\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other kinds of sentence vectors: Sentence BERT (S-BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "em1 = model.encode(s1, convert_to_tensor=True)\n",
    "em2 = model.encode(s2, convert_to_tensor=True)\n",
    "em3 = model.encode(s3, convert_to_tensor=True)\n",
    "em4 = model.encode(s4, convert_to_tensor=True)\n",
    "em5 = model.encode(s5, convert_to_tensor=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allsentences = [s1, s2, s3, s4, s5]\n",
    "allsbert = [em1, em2, em3, em4, em5]\n",
    "allw2v = [sent1, sent2, sent3, sent4, sent5]\n",
    "for i in range(len(allsentences)):\n",
    "    for j in range(len(allsentences)):\n",
    "        print(allsentences[i] + \" VS. \" + allsentences[j], end=\" ||| \")\n",
    "        print(f'{float(util.cos_sim(allsbert[i], allsbert[j])):.3f}', end=\" ||| \")\n",
    "        print(f'{1-cosine(allw2v[i], allw2v[j]):.3f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = [em1, em2, em3, em4, em5]\n",
    "vecwords = [\"dog\", \"cat\", \"CS\", \"programming\", \"mouse\"]\n",
    "\n",
    "    \n",
    "# Do  PCA to reduce to 2 dimensions\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "vectors2d = pca.fit(vecs).transform(vecs)\n",
    "\n",
    "# Again, ugly matplotlib code to create visualization\n",
    "i = 0\n",
    "for point, word in zip(vectors2d, vecwords):\n",
    "    plt.scatter(point[0], point[1], c='r')\n",
    "    \n",
    "    plt.annotate(\n",
    "            word, \n",
    "            xy=(point[0], point[1]),\n",
    "            xytext=(7, 6),\n",
    "            textcoords='offset points',\n",
    "            ha='left' ,\n",
    "            va='top',\n",
    "            size=\"medium\"\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
