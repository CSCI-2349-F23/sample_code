{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 10.1: Intro to Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing gensim and its dependencies and launching a Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``python3 -m pip install numpy``\n",
    "\n",
    "``python3 -m pip install scipy``\n",
    "\n",
    "``python3 -m pip install gensim``\n",
    "\n",
    "``python3 -m pip install scikit-learn``\n",
    "\n",
    "``jupyter notebook``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a pre-trained word2vec model\n",
    "\n",
    "You can get a pre-trained word2vec model built on billions words of Google newsfrom here:\n",
    "\n",
    "https://github.com/eyaler/word2vec-slim/blob/master/GoogleNews-vectors-negative300-SLIM.bin.gz\n",
    "\n",
    "Just click on the \"download\" icon next to where it says \"Raw\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Word2Vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and using the pre-trained word2vec model\n",
    "\n",
    "<b>Note: When you run the code below, it will take a minute or two to load the model!</b> Wait until you see <code>\"big model loaded\"</code> printed out below the cell. You can also check for the <code>*</code> in the brackets to the left of the cell you are executing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigmodel = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300-SLIM.bin.gz\", binary=True)\n",
    "print(\"big model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the word embedding (i.e., word vector) for any word that's in the model like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigmodel[\"dog\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not especially interesting because these numbers are not human interpretable. Instead we might like to look at how words are similar to each other, like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigmodel.similarity('dog', 'computer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigmodel.similarity(\"dog\", \"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigmodel.similarity(\"dog\", \"leash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigmodel.similarity(\"dog\", \"barking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigmodel.similarity(\"dog\", \"enhance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see what words are most similar to some word, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigmodel.most_similar(\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigmodel.most_similar(\"soccer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask the model to pick out the word that doesn't belong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigmodel.doesnt_match([\"fret\", \"neck\", \"string\", \"key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigmodel.doesnt_match([\"breakfast\", \"lunch\", \"dinner\", \"chair\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't always work the way you think it might. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigmodel.doesnt_match([\"set\", \"list\", \"dictionary\", \"elephant\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do cool things by combining word vectors. We can solve analogies just like on a standardized test.\n",
    "\n",
    "\"*Woman* is to *man* as ________ is to *boy*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigmodel.most_similar(positive=['woman', 'boy'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"*Woman* is to *man* as ________ is to *king*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigmodel.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class, I showed you some plots of word vectors, where the 300 dimensions had been projected down to 2 dimensions. Here we will use a dimensionality reduction method to reduce our word vectors to 2 dimensions so that we can visualize them. The code in the cell below will take word pairs, in which the first word is related in some way to the second word, and plot them in two dimensions. Execute this cell, and you should see a nice graph underneath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpairs = {\"Madrid\":\"Spain\", \"Paris\":\"France\",  \"Berlin\":\"Germany\", \"Beijing\":\"China\", \"Tokyo\":\"Japan\"}\n",
    "\n",
    "# Go get the word vectors for these words and \n",
    "# then store them so you can use them later on.\n",
    "vecwords = []  # stores the words above\n",
    "vecs = []      # stores the vectors for each word\n",
    "for k,v in wordpairs.items():\n",
    "    kvec = bigmodel[k]\n",
    "    vvec = bigmodel[v]\n",
    "    vecs.append(kvec)\n",
    "    vecwords.append(k)\n",
    "    vecs.append(vvec)\n",
    "    vecwords.append(v)\n",
    "    \n",
    "# PCA is a way to project multiple dimensions down to \n",
    "# fewer dimensions, which we are doing here so we can \n",
    "# visualize the word vectors.\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "vectors2d = pca.fit(vecs).transform(vecs)\n",
    "\n",
    "\n",
    "# This is just some ugly matplotlib code for plotting\n",
    "# the 2-D vectors and visualizing them with different colors.\n",
    "i = 2\n",
    "for point, word in zip(vectors2d, vecwords):\n",
    "    if i%2 == 0:\n",
    "        plt.scatter(point[0], point[1], c='r')\n",
    "    else:\n",
    "        plt.scatter(point[0], point[1], c='b')\n",
    "    i += 1\n",
    "    \n",
    "    plt.annotate(\n",
    "            word, \n",
    "            xy=(point[0], point[1]),\n",
    "            xytext=(7, 6),\n",
    "            textcoords='offset points',\n",
    "            ha='left' ,\n",
    "            va='top',\n",
    "            size=\"medium\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below does something similar: it takes two lists of words and plots one in blue and one in red. If the words in one list are very related to one another and the words in the other list are very related to one another, you should see the red and blue dots clustering in different parts of the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some words associated with 2 different categories: work and school\n",
    "\n",
    "vecwords1 = \"commute boss office paperwork\".split()  \n",
    "vecwords2 = \"teacher studying library exams\".split()\n",
    "vecs = []\n",
    "vecwords = []\n",
    "\n",
    "# Get their vectors\n",
    "for w in vecwords1:\n",
    "    v = bigmodel[w]\n",
    "    vecs.append(v)\n",
    "    vecwords.append(w)\n",
    "\n",
    "for w in vecwords2:\n",
    "    v = bigmodel[w]\n",
    "    vecs.append(v)\n",
    "    vecwords.append(w)\n",
    "\n",
    "    \n",
    "#tsne = TSNE(n_components=2, random_state=0)\n",
    "#vectors2d = tsne.fit_transform(vecs)\n",
    "\n",
    "# Do the PCA to reduce to 2 dimensions\n",
    "pca = PCA(n_components=2, whiten=True)\n",
    "vectors2d = pca.fit(vecs).transform(vecs)\n",
    "\n",
    "# Again, ugly matplotlib code to create visualization\n",
    "i = 0\n",
    "for point, word in zip(vectors2d, vecwords):\n",
    "    if i < len(vecwords1):\n",
    "        plt.scatter(point[0], point[1], c='r')\n",
    "    else:\n",
    "        plt.scatter(point[0], point[1], c='b')\n",
    "    i += 1\n",
    "    \n",
    "    plt.annotate(\n",
    "            word, \n",
    "            xy=(point[0], point[1]),\n",
    "            xytext=(7, 6),\n",
    "            textcoords='offset points',\n",
    "            ha='left' ,\n",
    "            va='top',\n",
    "            size=\"medium\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
